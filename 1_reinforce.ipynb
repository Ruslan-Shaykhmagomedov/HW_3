{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch (5 pts)\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"bash\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21ad4d31f30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\1_reinforce.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39m# Build a simple neural network that predicts policy logits. \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39m# Keep it simple: CartPole isn't worth deep architectures.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=3'>4</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(state_dim[\u001b[39m0\u001b[39m], \u001b[39m32\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=4'>5</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=5'>6</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=6'>7</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=7'>8</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m64\u001b[39m, \u001b[39m32\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=8'>9</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=9'>10</a>\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m32\u001b[39m, n_actions),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=10'>11</a>\u001b[0m     nn\u001b[39m.\u001b[39mReLU()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/1_reinforce.ipynb#ch0000006?line=11'>12</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(state_dim[0], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, n_actions),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
    "So, here gradient calculation is not needed.\n",
    "<br>\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "<br>\n",
    "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
    "<br>\n",
    "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
    "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "<br>\n",
    "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.Tensor(states))\n",
    "        prob = nn.functional.softmax(pred, dim=1)\n",
    "    return prob.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    cum_rewards = []\n",
    "    prev = 0\n",
    "    for r in reversed(rewards):\n",
    "        prev = r + gamma * prev\n",
    "        cum_rewards.append(prev)\n",
    "    cum_rewards.reverse()\n",
    "    return cum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over T } \\sum_{i=1}^T  G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over T } \\sum_{i=1}^T \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "Entropy Regularizer\n",
    "  $$ H = - {1 \\over T} \\sum_{i=1}^T  \\sum_{a \\in A} {\\pi_\\theta(a|s_i) \\cdot \\log \\pi_\\theta(a|s_i)}$$\n",
    "\n",
    "$T$ is session length\n",
    "\n",
    "So we optimize a linear combination of $- \\hat J$, $-H$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = - torch.sum(torch.sum(probs * log_probs, 1))\n",
    "    \n",
    "    J = torch.mean(log_probs_for_actions * cumulative_returns)\n",
    "    loss = -J + entropy_coef * entropy\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env))\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "monitor_env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session(monitor_env) for _ in range(100)]\n",
    "monitor_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\" + video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
