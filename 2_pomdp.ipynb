{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially observable Markov decision process (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/atari_util.py\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/0ccb0673965dd650d9b284e1ec90c2bfd82c8a94/week08_pomdp/env_pool.py\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kung-Fu, recurrent style\n",
    "\n",
    "In this notebook we'll once again train RL agent for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), this time using recurrent neural networks.\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop = lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42,42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 24, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv1 = nn.Conv2d(24, 24, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(24, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        temp = F.relu(self.hid(self.flatten(F.elu(self.conv2(F.elu(self.conv1(F.elu(self.conv0(obs_t)))))))))\n",
    "        \n",
    "        h_new, c_new = self.rnn(temp, prev_state)\n",
    "        logits = self.logits(h_new)\n",
    "        state_value = self.state_value(h_new)\n",
    "        \n",
    "        return (h_new, c_new), (logits, state_value)\n",
    "    \n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return (Variable(torch.zeros((batch_size, 128))),\n",
    "                Variable(torch.zeros((batch_size, 128))))\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "    \n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is not Variable \"\"\"\n",
    "        obs_t = Variable(torch.FloatTensor(np.array(obs_t)))\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action logits:\n",
      " tensor([[ 0.0504,  0.0471, -0.0888, -0.0953,  0.0467,  0.0721, -0.0449, -0.0488,\n",
      "         -0.0865, -0.0184,  0.0586,  0.0233,  0.0614, -0.0387]])\n",
      "state values:\n",
      " tensor([[0.0582]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done: break\n",
    "                \n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600.0, 300.0, 400.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print (rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\2_pomdp.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=4'>5</a>\u001b[0m video_names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m s:s\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.mp4\u001b[39m\u001b[39m\"\u001b[39m),os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39m./kungfu_videos/\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=6'>7</a>\u001b[0m HTML(\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=7'>8</a>\u001b[0m \u001b[39m<video width=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m640\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m height=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m480\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m controls>\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=8'>9</a>\u001b[0m \u001b[39m  <source src=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m type=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvideo/mp4\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=9'>10</a>\u001b[0m \u001b[39m</video>\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000014?line=10'>11</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m./kungfu_videos/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mvideo_names[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](img2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \",rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s_t,a_t) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over T} \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s_t,a_t) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ $\\hat J$ with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$, $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast everything into a variable\n",
    "    states = Variable(torch.FloatTensor(np.array(states)))   # shape: [batch_size, time, c, h, w]\n",
    "    actions = Variable(torch.IntTensor(np.array(actions)))   # shape: [batch_size, time]\n",
    "    rewards = Variable(torch.FloatTensor(np.array(rewards))) # shape: [batch_size, time]\n",
    "    is_not_done = Variable(torch.FloatTensor(is_not_done.astype('float32')))  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent. \n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "    \n",
    "    logits = [] # append logit sequence here\n",
    "    state_values = [] #append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "        \n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "        \n",
    "        memory, (logits_t, values_t) = agent(memory, obs_t)\n",
    "        \n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "        \n",
    "        \n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim = -1)\n",
    "    \n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective. \n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0 # policy objective as in the formula for J_hat\n",
    "    \n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "    \n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "    \n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        V_t = state_values[:, t]                           # current state values\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]          # log-probability of a_t in s_t\n",
    "        \n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "        \n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += ((r_t + gamma * V_next - V_t) ** 2).mean()\n",
    "        \n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = cumulative_returns - V_t\n",
    "        advantage = advantage.detach()\n",
    "        \n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += (logpi_a_s_t * advantage).mean()\n",
    "        \n",
    "    #regularize with entropy\n",
    "    entropy_reg = -(probas * logprobas).sum(-1).mean()\n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "           value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "    \n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\2_pomdp.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000023?line=1'>2</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(pool\u001b[39m.\u001b[39mprev_memory_states)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000023?line=2'>3</a>\u001b[0m rollout_obs, rollout_actions, rollout_rewards, rollout_mask \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39minteract(\u001b[39m10\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000023?line=4'>5</a>\u001b[0m train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)\n",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\2_pomdp.ipynb Cell 23'\u001b[0m in \u001b[0;36mtrain_on_rollout\u001b[1;34m(states, actions, rewards, is_not_done, prev_memory_states, gamma)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=36'>37</a>\u001b[0m logprobas \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=38'>39</a>\u001b[0m \u001b[39m# select log-probabilities for chosen actions, log pi(a_i|s_i)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=39'>40</a>\u001b[0m actions_one_hot \u001b[39m=\u001b[39m to_one_hot(actions, n_actions)\u001b[39m.\u001b[39mview(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=40'>41</a>\u001b[0m     actions\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], actions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], n_actions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=41'>42</a>\u001b[0m logprobas_for_actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(logprobas \u001b[39m*\u001b[39m actions_one_hot, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=43'>44</a>\u001b[0m \u001b[39m# Now let's compute two loss components:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=44'>45</a>\u001b[0m \u001b[39m# 1) Policy gradient objective. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=45'>46</a>\u001b[0m \u001b[39m# Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000022?line=46'>47</a>\u001b[0m \u001b[39m# it's okay to use loops if you want\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\2_pomdp.ipynb Cell 22'\u001b[0m in \u001b[0;36mto_one_hot\u001b[1;34m(y, n_dims)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000021?line=1'>2</a>\u001b[0m \u001b[39m\"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000021?line=2'>3</a>\u001b[0m y_tensor \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(y, Variable) \u001b[39melse\u001b[39;00m y\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000021?line=3'>4</a>\u001b[0m y_tensor \u001b[39m=\u001b[39m y_tensor\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mLongTensor)\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000021?line=4'>5</a>\u001b[0m n_dims \u001b[39m=\u001b[39m n_dims \u001b[39mif\u001b[39;00m n_dims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mint\u001b[39m(torch\u001b[39m.\u001b[39mmax(y_tensor)) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000021?line=5'>6</a>\u001b[0m y_one_hot \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(y_tensor\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], n_dims)\u001b[39m.\u001b[39mscatter_(\u001b[39m1\u001b[39m, y_tensor, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ewma' from 'pandas' (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\2_pomdp.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m clear_output\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000025?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m trange\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000025?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m ewma\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000025?line=3'>4</a>\u001b[0m rewards_history \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000025?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlr_scheduler\u001b[39;00m \u001b[39mimport\u001b[39;00m StepLR\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ewma' from 'pandas' (c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import ewma\n",
    "rewards_history = []\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "sched = StepLR(opt, step_size=20000, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Documents\\GitHub\\HW_3\\2_pomdp.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m trange(\u001b[39m15000\u001b[39m):  \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000026?line=1'>2</a>\u001b[0m     sched\u001b[39m.\u001b[39mstep()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000026?line=2'>3</a>\u001b[0m     memory \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(pool\u001b[39m.\u001b[39mprev_memory_states)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/GitHub/HW_3/2_pomdp.ipynb#ch0000026?line=3'>4</a>\u001b[0m     rollout_obs, rollout_actions, rollout_rewards, rollout_mask \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39minteract(\u001b[39m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sched' is not defined"
     ]
    }
   ],
   "source": [
    "for i in trange(15000):  \n",
    "    sched.step()\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(20)\n",
    "    train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)    \n",
    "    \n",
    "    if i % 200 == 0: \n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(ewma(np.array(rewards_history),span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 25000:\n",
    "            print(\"Your agent has just passed the 25000 threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/56069467) and maybe use a smaller network\n",
    "* Us. Or PyTorch developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
